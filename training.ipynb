{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to C:\\Users\\LENOVO/.cache\\torch\\hub\\checkpoints\\alexnet-owt-7be5be79.pth\n",
      "100%|██████████| 233M/233M [01:53<00:00, 2.15MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available\n",
      "Epoch 1/20: Loss: 638.0319, Accuracy: 0.8199, Precision: 0.8192, Recall: 0.8201, F1 score: 0.8194\n",
      "Validation: Loss: 77.8796, Accuracy: 0.9055, Precision: 0.9107, Recall: 0.9062, F1: 0.9039\n",
      "Epoch 2/20: Loss: 466.7012, Accuracy: 0.8645, Precision: 0.8640, Recall: 0.8647, F1 score: 0.8643\n",
      "Validation: Loss: 62.6348, Accuracy: 0.9246, Precision: 0.9285, Recall: 0.9249, F1: 0.9233\n",
      "Epoch 3/20: Loss: 443.4666, Accuracy: 0.8727, Precision: 0.8724, Recall: 0.8730, F1 score: 0.8727\n",
      "Validation: Loss: 65.2028, Accuracy: 0.9195, Precision: 0.9272, Recall: 0.9204, F1: 0.9192\n",
      "Epoch 4/20: Loss: 431.2574, Accuracy: 0.8785, Precision: 0.8782, Recall: 0.8786, F1 score: 0.8784\n",
      "Validation: Loss: 62.9648, Accuracy: 0.9275, Precision: 0.9310, Recall: 0.9276, F1: 0.9265\n",
      "Epoch 5/20: Loss: 429.2622, Accuracy: 0.8814, Precision: 0.8813, Recall: 0.8816, F1 score: 0.8814\n",
      "Validation: Loss: 59.3454, Accuracy: 0.9317, Precision: 0.9353, Recall: 0.9318, F1: 0.9309\n",
      "Epoch 6/20: Loss: 425.5922, Accuracy: 0.8840, Precision: 0.8839, Recall: 0.8842, F1 score: 0.8840\n",
      "Validation: Loss: 55.1192, Accuracy: 0.9332, Precision: 0.9361, Recall: 0.9339, F1: 0.9332\n",
      "Epoch 7/20: Loss: 422.8765, Accuracy: 0.8848, Precision: 0.8847, Recall: 0.8850, F1 score: 0.8848\n",
      "Validation: Loss: 53.5775, Accuracy: 0.9351, Precision: 0.9380, Recall: 0.9357, F1: 0.9337\n",
      "Epoch 8/20: Loss: 328.9323, Accuracy: 0.9055, Precision: 0.9054, Recall: 0.9058, F1 score: 0.9056\n",
      "Validation: Loss: 41.5279, Accuracy: 0.9492, Precision: 0.9503, Recall: 0.9491, F1: 0.9486\n",
      "Epoch 9/20: Loss: 318.8550, Accuracy: 0.9073, Precision: 0.9071, Recall: 0.9075, F1 score: 0.9073\n",
      "Validation: Loss: 40.0486, Accuracy: 0.9504, Precision: 0.9515, Recall: 0.9504, F1: 0.9500\n",
      "Epoch 10/20: Loss: 308.2074, Accuracy: 0.9100, Precision: 0.9100, Recall: 0.9103, F1 score: 0.9101\n",
      "Validation: Loss: 38.4399, Accuracy: 0.9540, Precision: 0.9550, Recall: 0.9542, F1: 0.9534\n",
      "Epoch 11/20: Loss: 300.7191, Accuracy: 0.9123, Precision: 0.9122, Recall: 0.9125, F1 score: 0.9123\n",
      "Validation: Loss: 38.7178, Accuracy: 0.9527, Precision: 0.9532, Recall: 0.9530, F1: 0.9521\n",
      "Epoch 12/20: Loss: 302.5263, Accuracy: 0.9110, Precision: 0.9109, Recall: 0.9113, F1 score: 0.9110\n",
      "Validation: Loss: 38.1501, Accuracy: 0.9543, Precision: 0.9553, Recall: 0.9544, F1: 0.9539\n",
      "Epoch 13/20: Loss: 294.2942, Accuracy: 0.9122, Precision: 0.9121, Recall: 0.9124, F1 score: 0.9122\n",
      "Validation: Loss: 37.8270, Accuracy: 0.9537, Precision: 0.9552, Recall: 0.9537, F1: 0.9534\n",
      "Epoch 14/20: Loss: 287.3640, Accuracy: 0.9141, Precision: 0.9141, Recall: 0.9143, F1 score: 0.9142\n",
      "Validation: Loss: 38.1643, Accuracy: 0.9525, Precision: 0.9534, Recall: 0.9526, F1: 0.9520\n",
      "Epoch 15/20: Loss: 286.0732, Accuracy: 0.9149, Precision: 0.9148, Recall: 0.9151, F1 score: 0.9149\n",
      "Validation: Loss: 38.1760, Accuracy: 0.9550, Precision: 0.9554, Recall: 0.9550, F1: 0.9545\n",
      "Epoch 16/20: Loss: 288.8934, Accuracy: 0.9144, Precision: 0.9143, Recall: 0.9146, F1 score: 0.9144\n",
      "Validation: Loss: 37.6568, Accuracy: 0.9546, Precision: 0.9550, Recall: 0.9547, F1: 0.9540\n",
      "Epoch 17/20: Loss: 283.6204, Accuracy: 0.9154, Precision: 0.9154, Recall: 0.9156, F1 score: 0.9154\n",
      "Validation: Loss: 38.2079, Accuracy: 0.9529, Precision: 0.9535, Recall: 0.9532, F1: 0.9524\n",
      "Epoch 18/20: Loss: 283.9117, Accuracy: 0.9153, Precision: 0.9152, Recall: 0.9155, F1 score: 0.9153\n",
      "Validation: Loss: 36.5253, Accuracy: 0.9562, Precision: 0.9566, Recall: 0.9564, F1: 0.9557\n",
      "Epoch 19/20: Loss: 278.6053, Accuracy: 0.9165, Precision: 0.9164, Recall: 0.9167, F1 score: 0.9165\n",
      "Validation: Loss: 36.5635, Accuracy: 0.9561, Precision: 0.9566, Recall: 0.9563, F1: 0.9557\n",
      "Epoch 20/20: Loss: 279.7469, Accuracy: 0.9176, Precision: 0.9176, Recall: 0.9178, F1 score: 0.9177\n",
      "Validation: Loss: 37.1581, Accuracy: 0.9546, Precision: 0.9555, Recall: 0.9546, F1: 0.9541\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average=\"macro\")\n",
    "    recall = recall_score(y_true, y_pred, average=\"macro\")\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder('train', transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, num_workers=8)\n",
    "val_data = datasets.ImageFolder('valid', transform=transform)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=True, num_workers=8)\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"runs/alexnet\")\n",
    "\n",
    "model = models.alexnet(pretrained=True)\n",
    "for par in model.parameters():\n",
    "    par.requires_grad=False\n",
    "\n",
    "model.classifier[6] = nn.Linear(model.classifier[6].in_features, 38)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available\")\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    accuracy, precision, recall, f1 = calculate_metrics(all_labels, all_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{num_epochs}: Loss: {total_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 score: {f1:.4f}\")\n",
    "    writer.add_scalar(\"Training Loss\", total_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy\", accuracy, epoch)\n",
    "    writer.add_scalar(\"Precision\", precision, epoch)\n",
    "    writer.add_scalar(\"Recall\", recall, epoch)\n",
    "    writer.add_scalar(\"F1 Score\", f1, epoch)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "    val_accuracy, val_precision, val_recall, val_f1 = calculate_metrics(val_labels, val_preds)\n",
    "    print(f\"Validation: Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}\")\n",
    "    writer.add_scalar(\"Validation Loss\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Validation Accuracy\", val_accuracy, epoch)\n",
    "    writer.add_scalar(\"Validation Precision\", val_precision, epoch)\n",
    "    writer.add_scalar(\"Validation Recall\", val_recall, epoch)\n",
    "    writer.add_scalar(\"Validation F1 Score\", val_f1, epoch)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'Alexnet_model.pth')\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available\n",
      "Epoch 1/20: Loss: 2857.2501, Accuracy: 0.2748, Precision: 0.2481, Recall: 0.2743, F1 score: 0.2473\n",
      "Validation: Loss: 610.8152, Accuracy: 0.3597, Precision: 0.3546, Recall: 0.3599, F1: 0.3399\n",
      "Epoch 2/20: Loss: 2221.0530, Accuracy: 0.4123, Precision: 0.3942, Recall: 0.4127, F1 score: 0.3956\n",
      "Validation: Loss: 518.8904, Accuracy: 0.4470, Precision: 0.4445, Recall: 0.4488, F1: 0.4375\n",
      "Epoch 3/20: Loss: 1943.8122, Accuracy: 0.4750, Precision: 0.4607, Recall: 0.4759, F1 score: 0.4638\n",
      "Validation: Loss: 469.8042, Accuracy: 0.4952, Precision: 0.4899, Recall: 0.4967, F1: 0.4859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x0000025C15DBB560>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1562, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x0000025C15DBB560>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1562, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: Loss: 1765.1591, Accuracy: 0.5214, Precision: 0.5095, Recall: 0.5226, F1 score: 0.5128\n",
      "Validation: Loss: 432.7563, Accuracy: 0.5299, Precision: 0.5180, Recall: 0.5318, F1: 0.5196\n",
      "Epoch 5/20: Loss: 1630.5730, Accuracy: 0.5532, Precision: 0.5435, Recall: 0.5544, F1 score: 0.5463\n",
      "Validation: Loss: 403.1619, Accuracy: 0.5581, Precision: 0.5519, Recall: 0.5607, F1: 0.5522\n",
      "Epoch 6/20: Loss: 1540.0691, Accuracy: 0.5753, Precision: 0.5668, Recall: 0.5765, F1 score: 0.5695\n",
      "Validation: Loss: 389.3841, Accuracy: 0.5717, Precision: 0.5700, Recall: 0.5731, F1: 0.5622\n",
      "Epoch 7/20: Loss: 1462.3181, Accuracy: 0.5938, Precision: 0.5861, Recall: 0.5950, F1 score: 0.5886\n",
      "Validation: Loss: 375.2482, Accuracy: 0.5868, Precision: 0.5961, Recall: 0.5886, F1: 0.5847\n",
      "Epoch 8/20: Loss: 1314.0995, Accuracy: 0.6354, Precision: 0.6296, Recall: 0.6367, F1 score: 0.6312\n",
      "Validation: Loss: 347.1352, Accuracy: 0.6156, Precision: 0.6071, Recall: 0.6169, F1: 0.6096\n",
      "Epoch 9/20: Loss: 1289.7757, Accuracy: 0.6425, Precision: 0.6367, Recall: 0.6437, F1 score: 0.6385\n",
      "Validation: Loss: 342.9317, Accuracy: 0.6193, Precision: 0.6144, Recall: 0.6207, F1: 0.6158\n",
      "Epoch 10/20: Loss: 1273.9769, Accuracy: 0.6470, Precision: 0.6417, Recall: 0.6483, F1 score: 0.6432\n",
      "Validation: Loss: 341.7492, Accuracy: 0.6220, Precision: 0.6176, Recall: 0.6230, F1: 0.6181\n",
      "Epoch 11/20: Loss: 1261.2945, Accuracy: 0.6514, Precision: 0.6460, Recall: 0.6526, F1 score: 0.6475\n",
      "Validation: Loss: 339.4127, Accuracy: 0.6207, Precision: 0.6164, Recall: 0.6219, F1: 0.6168\n",
      "Epoch 12/20: Loss: 1249.8775, Accuracy: 0.6518, Precision: 0.6467, Recall: 0.6530, F1 score: 0.6481\n",
      "Validation: Loss: 336.1476, Accuracy: 0.6276, Precision: 0.6230, Recall: 0.6290, F1: 0.6233\n",
      "Epoch 13/20: Loss: 1245.0038, Accuracy: 0.6543, Precision: 0.6490, Recall: 0.6556, F1 score: 0.6507\n",
      "Validation: Loss: 335.1952, Accuracy: 0.6290, Precision: 0.6235, Recall: 0.6302, F1: 0.6248\n",
      "Epoch 14/20: Loss: 1233.9420, Accuracy: 0.6578, Precision: 0.6528, Recall: 0.6591, F1 score: 0.6542\n",
      "Validation: Loss: 333.5130, Accuracy: 0.6303, Precision: 0.6245, Recall: 0.6318, F1: 0.6257\n",
      "Epoch 15/20: Loss: 1218.3293, Accuracy: 0.6615, Precision: 0.6562, Recall: 0.6628, F1 score: 0.6578\n",
      "Validation: Loss: 330.2568, Accuracy: 0.6365, Precision: 0.6309, Recall: 0.6379, F1: 0.6325\n",
      "Epoch 16/20: Loss: 1215.7297, Accuracy: 0.6625, Precision: 0.6574, Recall: 0.6637, F1 score: 0.6590\n",
      "Validation: Loss: 330.8755, Accuracy: 0.6343, Precision: 0.6289, Recall: 0.6357, F1: 0.6306\n",
      "Epoch 17/20: Loss: 1213.0529, Accuracy: 0.6625, Precision: 0.6578, Recall: 0.6636, F1 score: 0.6591\n",
      "Validation: Loss: 328.9668, Accuracy: 0.6379, Precision: 0.6317, Recall: 0.6391, F1: 0.6335\n",
      "Epoch 18/20: Loss: 1210.0519, Accuracy: 0.6629, Precision: 0.6579, Recall: 0.6642, F1 score: 0.6594\n",
      "Validation: Loss: 330.8632, Accuracy: 0.6337, Precision: 0.6280, Recall: 0.6349, F1: 0.6299\n",
      "Epoch 19/20: Loss: 1212.9920, Accuracy: 0.6621, Precision: 0.6571, Recall: 0.6633, F1 score: 0.6585\n",
      "Validation: Loss: 329.5551, Accuracy: 0.6360, Precision: 0.6293, Recall: 0.6373, F1: 0.6314\n",
      "Epoch 20/20: Loss: 1208.1423, Accuracy: 0.6627, Precision: 0.6580, Recall: 0.6639, F1 score: 0.6595\n",
      "Validation: Loss: 330.3012, Accuracy: 0.6348, Precision: 0.6292, Recall: 0.6361, F1: 0.6310\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average=\"macro\")\n",
    "    recall = recall_score(y_true, y_pred, average=\"macro\")\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.Grayscale(num_output_channels=1)\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "train_data = datasets.ImageFolder('train', transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, num_workers=8)\n",
    "val_data = datasets.ImageFolder('valid', transform=transform)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=True, num_workers=8)\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"runs/LeNet\")\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available\")\n",
    "\n",
    "# Move model to device\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels= 6,kernel_size= 5)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2,stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6,out_channels= 16,kernel_size= 5)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16,out_channels= 120,kernel_size= 5)\n",
    "        self.fc1=nn.Linear(120,84)\n",
    "        self.fc2=nn.Linear(84,38)\n",
    "    def forward(self,X):\n",
    "        X=torch.tanh(self.conv1(X))\n",
    "        X=self.pool(X)\n",
    "        X=torch.tanh(self.conv2(X))\n",
    "        X=self.pool(X)\n",
    "        X=torch.tanh(self.conv3(X))\n",
    "        X = X.view(-1, 120)\n",
    "        X=torch.tanh(self.fc1(X))\n",
    "        X=self.fc2(X)\n",
    "        return X\n",
    "model=LeNet()\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    accuracy, precision, recall, f1 = calculate_metrics(all_labels, all_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{num_epochs}: Loss: {total_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 score: {f1:.4f}\")\n",
    "    writer.add_scalar(\"Training Loss\", total_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy\", accuracy, epoch)\n",
    "    writer.add_scalar(\"Precision\", precision, epoch)\n",
    "    writer.add_scalar(\"Recall\", recall, epoch)\n",
    "    writer.add_scalar(\"F1 Score\", f1, epoch)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "    val_accuracy, val_precision, val_recall, val_f1 = calculate_metrics(val_labels, val_preds)\n",
    "    print(f\"Validation: Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}\")\n",
    "    writer.add_scalar(\"Validation Loss\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Validation Accuracy\", val_accuracy, epoch)\n",
    "    writer.add_scalar(\"Validation Precision\", val_precision, epoch)\n",
    "    writer.add_scalar(\"Validation Recall\", val_recall, epoch)\n",
    "    writer.add_scalar(\"Validation F1 Score\", val_f1, epoch)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'LeNet_model.pth')\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average=\"macro\")\n",
    "    recall = recall_score(y_true, y_pred, average=\"macro\")\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "train_data = datasets.ImageFolder('train', transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, num_workers=8)\n",
    "val_data = datasets.ImageFolder('valid', transform=transform)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=True, num_workers=8)\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"runs/efficientnet_b0\")\n",
    "\n",
    "model = models.efficientnet_b0(pretrained=True)\n",
    "for par in model.parameters():\n",
    "    par.requires_grad=False\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(model.classifier[1].in_features, 38)  # Adjust number of output classes\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available\")\n",
    "\n",
    "# Move model to device\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    accuracy, precision, recall, f1 = calculate_metrics(all_labels, all_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{num_epochs}: Loss: {total_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 score: {f1:.4f}\")\n",
    "    writer.add_scalar(\"Training Loss\", total_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy\", accuracy, epoch)\n",
    "    writer.add_scalar(\"Precision\", precision, epoch)\n",
    "    writer.add_scalar(\"Recall\", recall, epoch)\n",
    "    writer.add_scalar(\"F1 Score\", f1, epoch)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "    val_accuracy, val_precision, val_recall, val_f1 = calculate_metrics(val_labels, val_preds)\n",
    "    print(f\"Validation: Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}\")\n",
    "    writer.add_scalar(\"Validation Loss\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Validation Accuracy\", val_accuracy, epoch)\n",
    "    writer.add_scalar(\"Validation Precision\", val_precision, epoch)\n",
    "    writer.add_scalar(\"Validation Recall\", val_recall, epoch)\n",
    "    writer.add_scalar(\"Validation F1 Score\", val_f1, epoch)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'efficientnet_b0_model.pth')\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ShuffleNet_V2_X1_0_Weights.IMAGENET1K_V1`. You can also use `weights=ShuffleNet_V2_X1_0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/shufflenetv2_x1-5666bf0f80.pth\" to C:\\Users\\LENOVO/.cache\\torch\\hub\\checkpoints\\shufflenetv2_x1-5666bf0f80.pth\n",
      "100%|██████████| 8.79M/8.79M [00:02<00:00, 3.27MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch 1/20: Loss: 2700.3600, Accuracy: 0.7550, Precision: 0.7916, Recall: 0.7518, F1: 0.7567\n",
      "Validation: Loss: 435.4503, Accuracy: 0.8746, Precision: 0.8780, Recall: 0.8734, F1: 0.8697\n",
      "Epoch 2/20: Loss: 1334.8693, Accuracy: 0.8822, Precision: 0.8813, Recall: 0.8814, F1: 0.8786\n",
      "Validation: Loss: 243.9134, Accuracy: 0.8982, Precision: 0.8987, Recall: 0.8973, F1: 0.8950\n",
      "Epoch 3/20: Loss: 855.2797, Accuracy: 0.9010, Precision: 0.8997, Recall: 0.9005, F1: 0.8989\n",
      "Validation: Loss: 169.5827, Accuracy: 0.9163, Precision: 0.9165, Recall: 0.9157, F1: 0.9146\n",
      "Epoch 4/20: Loss: 636.4080, Accuracy: 0.9136, Precision: 0.9125, Recall: 0.9132, F1: 0.9121\n",
      "Validation: Loss: 129.0395, Accuracy: 0.9281, Precision: 0.9276, Recall: 0.9276, F1: 0.9267\n",
      "Epoch 5/20: Loss: 507.8490, Accuracy: 0.9241, Precision: 0.9232, Recall: 0.9238, F1: 0.9231\n",
      "Validation: Loss: 107.4477, Accuracy: 0.9342, Precision: 0.9335, Recall: 0.9336, F1: 0.9331\n",
      "Epoch 6/20: Loss: 429.0293, Accuracy: 0.9304, Precision: 0.9297, Recall: 0.9301, F1: 0.9296\n",
      "Validation: Loss: 90.8756, Accuracy: 0.9398, Precision: 0.9397, Recall: 0.9395, F1: 0.9389\n",
      "Epoch 7/20: Loss: 375.2176, Accuracy: 0.9361, Precision: 0.9354, Recall: 0.9359, F1: 0.9354\n",
      "Validation: Loss: 80.3874, Accuracy: 0.9461, Precision: 0.9456, Recall: 0.9456, F1: 0.9452\n",
      "Epoch 8/20: Loss: 349.4336, Accuracy: 0.9377, Precision: 0.9370, Recall: 0.9374, F1: 0.9370\n",
      "Validation: Loss: 79.3017, Accuracy: 0.9458, Precision: 0.9456, Recall: 0.9453, F1: 0.9450\n",
      "Epoch 9/20: Loss: 346.8975, Accuracy: 0.9380, Precision: 0.9374, Recall: 0.9378, F1: 0.9374\n",
      "Validation: Loss: 77.2668, Accuracy: 0.9458, Precision: 0.9454, Recall: 0.9454, F1: 0.9451\n",
      "Epoch 10/20: Loss: 341.3292, Accuracy: 0.9390, Precision: 0.9384, Recall: 0.9388, F1: 0.9384\n",
      "Validation: Loss: 77.9802, Accuracy: 0.9453, Precision: 0.9448, Recall: 0.9448, F1: 0.9445\n",
      "Epoch 11/20: Loss: 336.8873, Accuracy: 0.9405, Precision: 0.9399, Recall: 0.9403, F1: 0.9399\n",
      "Validation: Loss: 77.8712, Accuracy: 0.9443, Precision: 0.9441, Recall: 0.9439, F1: 0.9435\n",
      "Epoch 12/20: Loss: 332.2019, Accuracy: 0.9415, Precision: 0.9410, Recall: 0.9412, F1: 0.9409\n",
      "Validation: Loss: 76.7903, Accuracy: 0.9454, Precision: 0.9450, Recall: 0.9448, F1: 0.9445\n",
      "Epoch 13/20: Loss: 329.7620, Accuracy: 0.9410, Precision: 0.9404, Recall: 0.9407, F1: 0.9403\n",
      "Validation: Loss: 75.2928, Accuracy: 0.9460, Precision: 0.9453, Recall: 0.9456, F1: 0.9452\n",
      "Epoch 14/20: Loss: 325.1680, Accuracy: 0.9419, Precision: 0.9413, Recall: 0.9416, F1: 0.9413\n",
      "Validation: Loss: 73.4247, Accuracy: 0.9468, Precision: 0.9465, Recall: 0.9464, F1: 0.9460\n",
      "Epoch 15/20: Loss: 325.2419, Accuracy: 0.9413, Precision: 0.9407, Recall: 0.9411, F1: 0.9407\n",
      "Validation: Loss: 73.2085, Accuracy: 0.9492, Precision: 0.9490, Recall: 0.9489, F1: 0.9485\n",
      "Epoch 16/20: Loss: 325.3368, Accuracy: 0.9418, Precision: 0.9413, Recall: 0.9416, F1: 0.9413\n",
      "Validation: Loss: 73.5358, Accuracy: 0.9482, Precision: 0.9475, Recall: 0.9479, F1: 0.9474\n",
      "Epoch 17/20: Loss: 325.5619, Accuracy: 0.9406, Precision: 0.9401, Recall: 0.9404, F1: 0.9401\n",
      "Validation: Loss: 72.6548, Accuracy: 0.9495, Precision: 0.9492, Recall: 0.9491, F1: 0.9488\n",
      "Epoch 18/20: Loss: 323.9518, Accuracy: 0.9411, Precision: 0.9406, Recall: 0.9409, F1: 0.9405\n",
      "Validation: Loss: 73.4505, Accuracy: 0.9472, Precision: 0.9466, Recall: 0.9469, F1: 0.9464\n",
      "Epoch 19/20: Loss: 323.1966, Accuracy: 0.9405, Precision: 0.9399, Recall: 0.9403, F1: 0.9399\n",
      "Validation: Loss: 73.0111, Accuracy: 0.9484, Precision: 0.9480, Recall: 0.9481, F1: 0.9478\n",
      "Epoch 20/20: Loss: 322.9951, Accuracy: 0.9409, Precision: 0.9403, Recall: 0.9407, F1: 0.9403\n",
      "Validation: Loss: 72.8386, Accuracy: 0.9495, Precision: 0.9493, Recall: 0.9491, F1: 0.9488\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average=\"macro\")\n",
    "    recall = recall_score(y_true, y_pred, average=\"macro\")\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Data transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "train_data = datasets.ImageFolder('train', transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, num_workers=8)\n",
    "val_data = datasets.ImageFolder('valid', transform=transform)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=True, num_workers=8)\n",
    "\n",
    "# TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=\"runs/shufflenet_v2\")\n",
    "\n",
    "# Load pretrained ShuffleNetV2\n",
    "model = models.shufflenet_v2_x1_0(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(model.fc.in_features, 38)  # Adjust number of output classes\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    accuracy, precision, recall, f1 = calculate_metrics(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{num_epochs}: Loss: {total_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "    writer.add_scalar(\"Training Loss\", total_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy\", accuracy, epoch)\n",
    "    writer.add_scalar(\"Precision\", precision, epoch)\n",
    "    writer.add_scalar(\"Recall\", recall, epoch)\n",
    "    writer.add_scalar(\"F1 Score\", f1, epoch)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    val_accuracy, val_precision, val_recall, val_f1 = calculate_metrics(val_labels, val_preds)\n",
    "    print(f\"Validation: Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}\")\n",
    "    writer.add_scalar(\"Validation Loss\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Validation Accuracy\", val_accuracy, epoch)\n",
    "    writer.add_scalar(\"Validation Precision\", val_precision, epoch)\n",
    "    writer.add_scalar(\"Validation Recall\", val_recall, epoch)\n",
    "    writer.add_scalar(\"Validation F1 Score\", val_f1, epoch)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'shufflenet_v2_model.pth')\n",
    "\n",
    "# Close TensorBoard writer\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available\n",
      "Epoch 1/20: Loss: 1719.5297, Accuracy: 0.5863, Precision: 0.5808, Recall: 0.5861, F1 score: 0.5826\n",
      "Validation: Loss: 415.4273, Accuracy: 0.6561, Precision: 0.7272, Recall: 0.6598, F1: 0.6395\n",
      "Epoch 2/20: Loss: 634.9696, Accuracy: 0.8237, Precision: 0.8227, Recall: 0.8239, F1 score: 0.8232\n",
      "Validation: Loss: 2528.2674, Accuracy: 0.7864, Precision: 0.8148, Recall: 0.7851, F1: 0.7845\n",
      "Epoch 3/20: Loss: 493.4263, Accuracy: 0.8604, Precision: 0.8599, Recall: 0.8607, F1 score: 0.8602\n",
      "Validation: Loss: 1829.8780, Accuracy: 0.8605, Precision: 0.8785, Recall: 0.8610, F1: 0.8624\n",
      "Epoch 4/20: Loss: 358.5760, Accuracy: 0.8974, Precision: 0.8972, Recall: 0.8976, F1 score: 0.8973\n",
      "Validation: Loss: 20100.7028, Accuracy: 0.9001, Precision: 0.9045, Recall: 0.8998, F1: 0.9001\n",
      "Epoch 5/20: Loss: 268.9303, Accuracy: 0.9215, Precision: 0.9214, Recall: 0.9216, F1 score: 0.9215\n",
      "Validation: Loss: 50778.4195, Accuracy: 0.9183, Precision: 0.9254, Recall: 0.9174, F1: 0.9181\n",
      "Epoch 6/20: Loss: 286.9542, Accuracy: 0.9187, Precision: 0.9186, Recall: 0.9188, F1 score: 0.9187\n",
      "Validation: Loss: 19379.1633, Accuracy: 0.9269, Precision: 0.9286, Recall: 0.9268, F1: 0.9264\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average=\"macro\")\n",
    "    recall = recall_score(y_true, y_pred, average=\"macro\")\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "train_data = datasets.ImageFolder('train', transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, num_workers=8)\n",
    "val_data = datasets.ImageFolder('valid', transform=transform)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=True, num_workers=8)\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"runs/Plant_disease_CNN\")\n",
    "\n",
    "class Plant_disease_CNN(nn.Module):\n",
    "    def __init__(self, num_classes=38):\n",
    "        super(Plant_disease_CNN, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            nn.Conv2d(64, 192, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            nn.Conv2d(192, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            # Extra layers for deeper feature extraction\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 6 * 6, 4000),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(4000),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4000, 4000),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(4000),\n",
    "            nn.Linear(4000, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = Plant_disease_CNN(num_classes=38)\n",
    "\n",
    "\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    accuracy, precision, recall, f1 = calculate_metrics(all_labels, all_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{num_epochs}: Loss: {total_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 score: {f1:.4f}\")\n",
    "    writer.add_scalar(\"Training Loss\", total_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy\", accuracy, epoch)\n",
    "    writer.add_scalar(\"Precision\", precision, epoch)\n",
    "    writer.add_scalar(\"Recall\", recall, epoch)\n",
    "    writer.add_scalar(\"F1 Score\", f1, epoch)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "    val_accuracy, val_precision, val_recall, val_f1 = calculate_metrics(val_labels, val_preds)\n",
    "    print(f\"Validation: Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}\")\n",
    "    writer.add_scalar(\"Validation Loss\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Validation Accuracy\", val_accuracy, epoch)\n",
    "    writer.add_scalar(\"Validation Precision\", val_precision, epoch)\n",
    "    writer.add_scalar(\"Validation Recall\", val_recall, epoch)\n",
    "    writer.add_scalar(\"Validation F1 Score\", val_f1, epoch)\n",
    "\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available\n",
      "Epoch 1/20: Loss: 3296.0747, Accuracy: 0.6716, Precision: 0.6675, Recall: 0.6716, F1 score: 0.6679\n",
      "Validation: Loss: 796.8983, Accuracy: 0.8345, Precision: 0.8578, Recall: 0.8337, F1: 0.8354\n",
      "Epoch 2/20: Loss: 2997.4088, Accuracy: 0.8825, Precision: 0.8820, Recall: 0.8825, F1 score: 0.8822\n",
      "Validation: Loss: 747.2046, Accuracy: 0.8700, Precision: 0.8853, Recall: 0.8699, F1: 0.8698\n",
      "Epoch 3/20: Loss: 2805.0564, Accuracy: 0.9543, Precision: 0.9541, Recall: 0.9542, F1 score: 0.9541\n",
      "Validation: Loss: 700.4998, Accuracy: 0.9719, Precision: 0.9720, Recall: 0.9719, F1: 0.9717\n",
      "Epoch 4/20: Loss: 2778.3398, Accuracy: 0.9695, Precision: 0.9694, Recall: 0.9695, F1 score: 0.9694\n",
      "Validation: Loss: 691.8987, Accuracy: 0.9776, Precision: 0.9774, Recall: 0.9775, F1: 0.9774\n",
      "Epoch 5/20: Loss: 2762.0398, Accuracy: 0.9781, Precision: 0.9780, Recall: 0.9782, F1 score: 0.9781\n",
      "Validation: Loss: 688.7205, Accuracy: 0.9821, Precision: 0.9820, Recall: 0.9820, F1: 0.9820\n",
      "Epoch 6/20: Loss: 2759.4802, Accuracy: 0.9799, Precision: 0.9798, Recall: 0.9799, F1 score: 0.9798\n",
      "Validation: Loss: 687.0571, Accuracy: 0.9830, Precision: 0.9830, Recall: 0.9829, F1: 0.9829\n",
      "Epoch 7/20: Loss: 2757.4968, Accuracy: 0.9800, Precision: 0.9799, Recall: 0.9799, F1 score: 0.9799\n",
      "Validation: Loss: 695.1187, Accuracy: 0.9801, Precision: 0.9801, Recall: 0.9799, F1: 0.9799\n",
      "Epoch 8/20: Loss: 2757.8231, Accuracy: 0.9802, Precision: 0.9801, Recall: 0.9802, F1 score: 0.9801\n",
      "Validation: Loss: 687.2977, Accuracy: 0.9837, Precision: 0.9836, Recall: 0.9836, F1: 0.9836\n",
      "Epoch 9/20: Loss: 2757.2549, Accuracy: 0.9805, Precision: 0.9804, Recall: 0.9805, F1 score: 0.9804\n",
      "Validation: Loss: 706.8583, Accuracy: 0.9804, Precision: 0.9805, Recall: 0.9801, F1: 0.9802\n",
      "Epoch 10/20: Loss: 2756.7987, Accuracy: 0.9805, Precision: 0.9803, Recall: 0.9805, F1 score: 0.9804\n",
      "Validation: Loss: 688.2083, Accuracy: 0.9827, Precision: 0.9825, Recall: 0.9826, F1: 0.9825\n",
      "Epoch 11/20: Loss: 2757.8397, Accuracy: 0.9797, Precision: 0.9795, Recall: 0.9797, F1 score: 0.9796\n",
      "Validation: Loss: 689.2558, Accuracy: 0.9817, Precision: 0.9816, Recall: 0.9815, F1: 0.9815\n",
      "Epoch 12/20: Loss: 2757.1133, Accuracy: 0.9802, Precision: 0.9801, Recall: 0.9802, F1 score: 0.9801\n",
      "Validation: Loss: 694.6085, Accuracy: 0.9813, Precision: 0.9812, Recall: 0.9811, F1: 0.9810\n",
      "Epoch 13/20: Loss: 2757.3521, Accuracy: 0.9799, Precision: 0.9798, Recall: 0.9799, F1 score: 0.9799\n",
      "Validation: Loss: 689.4714, Accuracy: 0.9830, Precision: 0.9829, Recall: 0.9829, F1: 0.9829\n",
      "Epoch 14/20: Loss: 2757.0409, Accuracy: 0.9801, Precision: 0.9800, Recall: 0.9801, F1 score: 0.9800\n",
      "Validation: Loss: 688.6857, Accuracy: 0.9817, Precision: 0.9817, Recall: 0.9815, F1: 0.9816\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 117\u001b[0m\n\u001b[0;32m    115\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m    116\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m--> 117\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m clip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m5.0\u001b[39m)\n\u001b[0;32m    119\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optimising the model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average=\"macro\")\n",
    "    recall = recall_score(y_true, y_pred, average=\"macro\")\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder('train', transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, num_workers=4)\n",
    "val_data = datasets.ImageFolder('valid', transform=transform)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"runs/Disease_classifier\")\n",
    "\n",
    "class Disease_classifier(nn.Module):\n",
    "    def __init__(self, num_classes=38):\n",
    "        super(Disease_classifier, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            nn.Conv2d(64, 192, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            nn.Conv2d(192, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 6 * 6, 4000),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(4000),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4000, 4000),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(4000),\n",
    "            nn.Linear(4000, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = Disease_classifier(num_classes=38)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=.50)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    accuracy, precision, recall, f1 = calculate_metrics(all_labels, all_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{num_epochs}: Loss: {total_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 score: {f1:.4f}\")\n",
    "    writer.add_scalar(\"Training Loss\", total_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy\", accuracy, epoch)\n",
    "    writer.add_scalar(\"Precision\", precision, epoch)\n",
    "    writer.add_scalar(\"Recall\", recall, epoch)\n",
    "    writer.add_scalar(\"F1 Score\", f1, epoch)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "    val_accuracy, val_precision, val_recall, val_f1 = calculate_metrics(val_labels, val_preds)\n",
    "    print(f\"Validation: Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}\")\n",
    "    writer.add_scalar(\"Validation Loss\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Validation Accuracy\", val_accuracy, epoch)\n",
    "    writer.add_scalar(\"Validation Precision\", val_precision, epoch)\n",
    "    writer.add_scalar(\"Validation Recall\", val_recall, epoch)\n",
    "    writer.add_scalar(\"Validation F1 Score\", val_f1, epoch)\n",
    "    \n",
    "    break\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'Disease_classifier_model.pth')\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'Disease_classifier_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Disease_classifier(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=18432, out_features=4000, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(4000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4000, out_features=4000, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(4000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Linear(in_features=4000, out_features=38, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: Loss: 2102.0331, Accuracy: 0.5783, Precision: 0.5720, Recall: 0.5786, F1 score: 0.5739\n",
      "Validation: Loss: 465.0744, Accuracy: 0.7114, Precision: 0.7950, Recall: 0.7109, F1: 0.7000\n",
      "Epoch 2/20: Loss: 1450.6397, Accuracy: 0.8024, Precision: 0.8011, Recall: 0.8026, F1 score: 0.8016\n",
      "Validation: Loss: 373.6571, Accuracy: 0.8562, Precision: 0.8765, Recall: 0.8568, F1: 0.8569\n",
      "Epoch 3/20: Loss: 1254.1983, Accuracy: 0.8716, Precision: 0.8710, Recall: 0.8717, F1 score: 0.8712\n",
      "Validation: Loss: 299.8428, Accuracy: 0.9162, Precision: 0.9210, Recall: 0.9162, F1: 0.9147\n",
      "Epoch 4/20: Loss: 1143.9595, Accuracy: 0.9071, Precision: 0.9070, Recall: 0.9071, F1 score: 0.9070\n",
      "Validation: Loss: 370.1141, Accuracy: 0.8472, Precision: 0.8904, Recall: 0.8501, F1: 0.8466\n",
      "Epoch 5/20: Loss: 1080.5482, Accuracy: 0.9257, Precision: 0.9256, Recall: 0.9256, F1 score: 0.9256\n",
      "Validation: Loss: 276.9079, Accuracy: 0.9411, Precision: 0.9453, Recall: 0.9409, F1: 0.9405\n",
      "Epoch 6/20: Loss: 1037.8634, Accuracy: 0.9389, Precision: 0.9389, Recall: 0.9388, F1 score: 0.9388\n",
      "Validation: Loss: 265.7364, Accuracy: 0.9434, Precision: 0.9483, Recall: 0.9436, F1: 0.9436\n",
      "Epoch 7/20: Loss: 1001.5087, Accuracy: 0.9501, Precision: 0.9500, Recall: 0.9500, F1 score: 0.9500\n",
      "Validation: Loss: 235.3692, Accuracy: 0.9657, Precision: 0.9688, Recall: 0.9644, F1: 0.9648\n",
      "Epoch 8/20: Loss: 975.4166, Accuracy: 0.9573, Precision: 0.9572, Recall: 0.9572, F1 score: 0.9572\n",
      "Validation: Loss: 267.4549, Accuracy: 0.9451, Precision: 0.9506, Recall: 0.9452, F1: 0.9447\n",
      "Epoch 9/20: Loss: 955.3445, Accuracy: 0.9618, Precision: 0.9617, Recall: 0.9618, F1 score: 0.9617\n",
      "Validation: Loss: 229.6821, Accuracy: 0.9688, Precision: 0.9706, Recall: 0.9685, F1: 0.9684\n",
      "Epoch 10/20: Loss: 936.6457, Accuracy: 0.9674, Precision: 0.9672, Recall: 0.9673, F1 score: 0.9672\n",
      "Validation: Loss: 229.3553, Accuracy: 0.9843, Precision: 0.9846, Recall: 0.9842, F1: 0.9842\n",
      "Epoch 11/20: Loss: 920.9823, Accuracy: 0.9713, Precision: 0.9712, Recall: 0.9712, F1 score: 0.9712\n",
      "Validation: Loss: 234.2607, Accuracy: 0.9725, Precision: 0.9751, Recall: 0.9718, F1: 0.9724\n",
      "Epoch 12/20: Loss: 907.8973, Accuracy: 0.9739, Precision: 0.9739, Recall: 0.9737, F1 score: 0.9738\n",
      "Validation: Loss: 221.7953, Accuracy: 0.9726, Precision: 0.9743, Recall: 0.9724, F1: 0.9725\n",
      "Epoch 13/20: Loss: 893.8625, Accuracy: 0.9777, Precision: 0.9776, Recall: 0.9776, F1 score: 0.9776\n",
      "Validation: Loss: 232.5823, Accuracy: 0.9799, Precision: 0.9808, Recall: 0.9797, F1: 0.9798\n",
      "Epoch 14/20: Loss: 883.9714, Accuracy: 0.9797, Precision: 0.9795, Recall: 0.9795, F1 score: 0.9795\n",
      "Validation: Loss: 207.7525, Accuracy: 0.9878, Precision: 0.9879, Recall: 0.9876, F1: 0.9876\n",
      "Epoch 15/20: Loss: 872.5863, Accuracy: 0.9827, Precision: 0.9826, Recall: 0.9826, F1 score: 0.9826\n",
      "Validation: Loss: 215.4212, Accuracy: 0.9873, Precision: 0.9877, Recall: 0.9870, F1: 0.9872\n",
      "Epoch 16/20: Loss: 868.7511, Accuracy: 0.9825, Precision: 0.9825, Recall: 0.9825, F1 score: 0.9825\n",
      "Validation: Loss: 205.9647, Accuracy: 0.9897, Precision: 0.9897, Recall: 0.9895, F1: 0.9896\n",
      "Epoch 17/20: Loss: 859.3434, Accuracy: 0.9848, Precision: 0.9848, Recall: 0.9847, F1 score: 0.9847\n",
      "Validation: Loss: 205.4138, Accuracy: 0.9890, Precision: 0.9890, Recall: 0.9888, F1: 0.9889\n",
      "Epoch 18/20: Loss: 854.0456, Accuracy: 0.9859, Precision: 0.9858, Recall: 0.9858, F1 score: 0.9858\n",
      "Validation: Loss: 213.4913, Accuracy: 0.9782, Precision: 0.9804, Recall: 0.9781, F1: 0.9783\n",
      "Epoch 19/20: Loss: 848.0191, Accuracy: 0.9874, Precision: 0.9873, Recall: 0.9873, F1 score: 0.9873\n",
      "Validation: Loss: 204.2469, Accuracy: 0.9882, Precision: 0.9884, Recall: 0.9880, F1: 0.9880\n",
      "Epoch 20/20: Loss: 845.0407, Accuracy: 0.9877, Precision: 0.9877, Recall: 0.9877, F1 score: 0.9877\n",
      "Validation: Loss: 202.6924, Accuracy: 0.9908, Precision: 0.9909, Recall: 0.9907, F1: 0.9907\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average=\"macro\")\n",
    "    recall = recall_score(y_true, y_pred, average=\"macro\")\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.2, 0.2)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder('train', transform=train_transform)\n",
    "val_data = datasets.ImageFolder('valid', transform=val_transform)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False, num_workers=8)\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"runs/Disease_classifier_optimized\")\n",
    "\n",
    "class DiseaseClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=38):\n",
    "        super(DiseaseClassifier, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 192, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "            nn.Conv2d(192, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 6 * 6, 2050),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(2050),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2050, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available\")\n",
    "\n",
    "model = DiseaseClassifier(num_classes=38).to(device)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "num_epochs = 20\n",
    "observation_epochs = 4   \n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_labels, all_preds = [], []\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), 1.0) \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    accuracy, precision, recall, f1 = calculate_metrics(all_labels, all_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{num_epochs}: Loss: {total_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 score: {f1:.4f}\")\n",
    "    writer.add_scalar(\"Training Loss\", total_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy\", accuracy, epoch)\n",
    "    writer.add_scalar(\"Precision\", precision, epoch)\n",
    "    writer.add_scalar(\"Recall\", recall, epoch)\n",
    "    writer.add_scalar(\"F1 Score\", f1, epoch)\n",
    "    \n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_labels, val_preds = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    val_accuracy, val_precision, val_recall, val_f1 = calculate_metrics(val_labels, val_preds)\n",
    "    print(f\"Validation: Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}\")\n",
    "    writer.add_scalar(\"Validation Loss\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Validation Accuracy\", val_accuracy, epoch)\n",
    "    writer.add_scalar(\"Validation Precision\", val_precision, epoch)\n",
    "    writer.add_scalar(\"Validation Recall\", val_recall, epoch)\n",
    "    writer.add_scalar(\"Validation F1 Score\", val_f1, epoch)\n",
    "\n",
    "    scheduler.step(val_loss)  \n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "        torch.save(model.state_dict(), 'Disease_classifier.pth') \n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "    \n",
    "    if early_stop_counter >= observation_epochs:\n",
    "        print(\"Early stopping triggered. Training stopped.\")\n",
    "        break\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiseaseClassifier(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=9216, out_features=2050, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(2050, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=2050, out_features=38, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'Disease_classifier.pth') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
